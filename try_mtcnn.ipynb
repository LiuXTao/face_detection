{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from face_detection.get_nets import PNet, RNet, ONet\n",
    "from face_detection.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n",
    "from face_detection.first_stage import run_first_stage\n",
    "from face_detection.visualization_utils import show_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONet(\n  (features): Sequential(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (prelu1): PReLU(num_parameters=32)\n    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (prelu2): PReLU(num_parameters=64)\n    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (prelu3): PReLU(num_parameters=64)\n    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (conv4): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n    (prelu4): PReLU(num_parameters=128)\n    (flatten): Flatten()\n    (conv5): Linear(in_features=1152, out_features=256, bias=True)\n    (drop5): Dropout(p=0.25)\n    (prelu5): PReLU(num_parameters=256)\n  )\n  (conv6_1): Linear(in_features=256, out_features=2, bias=True)\n  (conv6_2): Linear(in_features=256, out_features=4, bias=True)\n  (conv6_3): Linear(in_features=256, out_features=10, bias=True)\n)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "pnet = PNet()\n",
    "rnet = RNet()\n",
    "onet = ONet()\n",
    "onet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "\n",
    "# if the value is too low, the algorithm will use a lot of memory\n",
    "min_face_size = 15.0\n",
    "\n",
    "thresholds = [0.6, 0.7, 0.8]\n",
    "nms_thresholds = [0.7, 0.7, 0.7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "image = Image.open('images/office1.jpg')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales: ['0.80', '0.57', '0.40', '0.28', '0.20', '0.14', '0.10', '0.07', '0.05', '0.04']\nnumber of different scales: 10\n"
     ]
    }
   ],
   "source": [
    "width, height = image.size\n",
    "min_length = min(width, height)\n",
    "\n",
    "min_detection_size = 12\n",
    "factor = 0.707\n",
    "\n",
    "scales = []\n",
    "\n",
    "m = min_detection_size / min_face_size\n",
    "min_length *= m\n",
    "\n",
    "factor_count = 0\n",
    "while min_length > min_detection_size:\n",
    "    scales.append(m*factor**factor_count)\n",
    "    min_length *= factor\n",
    "    factor_count += 1\n",
    "    \n",
    "print(\"scales:\", ['{:.2f}'.format(s) for s in scales])\n",
    "print('number of different scales:', len(scales))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pyCase\\face_detection\\face_detection\\first_stage.py:32: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  img = Variable(torch.FloatTensor(_preprocess(img)), volatile=True)\nD:\\pyCase\\face_detection\\face_detection\\get_nets.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  a = F.softmax(self.conv4_1(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding_boxes: 1021\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = []\n",
    "\n",
    "# 运行PNet\n",
    "for s in scales:\n",
    "    boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "    bounding_boxes.append(boxes)\n",
    "\n",
    "bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "bounding_boxes = np.vstack(bounding_boxes)\n",
    "print('number of bounding_boxes:', len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding boxes: 1021\n"
     ]
    }
   ],
   "source": [
    "# NMS + calibration\n",
    "keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
    "bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "# use offsets predicted by pnet to transform bounding boxes\n",
    "bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "# shape[n_boxes, 5]\n",
    "\n",
    "bounding_boxes = convert_to_square(bounding_boxes)\n",
    "bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "print('number of bounding boxes:', len(bounding_boxes))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes=bounding_boxes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pyCase\\face_detection\\face_detection\\get_nets.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  a = F.softmax(self.conv5_1(x))\n"
     ]
    }
   ],
   "source": [
    "# RNet\n",
    "\n",
    "img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n",
    "output = rnet(img_boxes)\n",
    "offsets = output[0].data.numpy()\n",
    "probs = output[1].data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding_boxes: 170\n"
     ]
    }
   ],
   "source": [
    "keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "bounding_boxes = bounding_boxes[keep]\n",
    "bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "offsets = offsets[keep]\n",
    "print('number of bounding_boxes:', len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding_boxes: 93\n"
     ]
    }
   ],
   "source": [
    "# NMS + calibration\n",
    "keep = nms(bounding_boxes, nms_thresholds[1])\n",
    "bounding_boxes = bounding_boxes[keep]\n",
    "bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "bounding_boxes = convert_to_square(bounding_boxes)\n",
    "bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "print('number of bounding_boxes:', len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  after removing the cwd from sys.path.\nD:\\pyCase\\face_detection\\face_detection\\get_nets.py:133: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  a = F.softmax(self.conv6_1(x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ONet使用\n",
    "img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "img_boxes = Variable(torch.FloatTensor(img_boxes), volatile=True)\n",
    "output = onet(img_boxes)\n",
    "\n",
    "landmarks = output[0].data.numpy()\n",
    "offsets = output[1].data.numpy()\n",
    "probs = output[2].data.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding boxes: 71\n"
     ]
    }
   ],
   "source": [
    "keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "\n",
    "bounding_boxes = bounding_boxes[keep]\n",
    "bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "offsets = offsets[keep]\n",
    "landmarks = landmarks[keep]\n",
    "\n",
    "# compute landmark points\n",
    "width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n",
    "landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n",
    "print('number of bounding boxes:', len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes, landmarks).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bounding boxes: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nms + calibrate\n",
    "bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "keep = nms(bounding_boxes, nms_thresholds[2], mode='min')\n",
    "bounding_boxes = bounding_boxes[keep]\n",
    "landmarks = landmarks[keep]\n",
    "print(\"number of bounding boxes:\", len(bounding_boxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(image, bounding_boxes, landmarks).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
